{
    "key_": "nndeploy.dag.Graph",
    "name_": "LLM_Qwen2_0.5B",
    "developer_": "zuiren",
    "source_": "https://modelscope.cn/models/qwen/Qwen2-0.5B-Instruct/summary",
    "desc_": "Qwen2/0.5B is a compact LLM with 0.5 billion parameters",
    "device_type_": "kDeviceTypeCodeCpu:0",
    "is_dynamic_input_": false,
    "inputs_": [],
    "is_dynamic_output_": false,
    "outputs_": [],
    "is_graph_": true,
    "parallel_type_": "kParallelTypeNone",
    "is_inner_": false,
    "node_type_": "Intermediate",
    "is_time_profile_": false,
    "is_debug_": false,
    "is_external_stream_": false,
    "is_graph_node_share_stream_": true,
    "queue_max_size_": 16,
    "is_loop_max_flag_": true,
    "loop_count_": -1,
    "image_url_": [
        "template[http,modelscope]@https://template.cn/template.jpg"
    ],
    "video_url_": [
        "template[http,modelscope]@https://template.cn/template.mp4"
    ],
    "audio_url_": [
        "template[http,modelscope]@https://template.cn/template.mp3"
    ],
    "model_url_": [
        "modelscope@nndeploy/nndeploy:qwen/Qwen2-0.5B-Instruct/onnx/llm.onnx",
        "modelscope@nndeploy/nndeploy:qwen/Qwen2-0.5B-Instruct/onnx/llm.onnx.data",
        "modelscope@nndeploy/nndeploy:qwen/Qwen2-0.5B-Instruct/llm_config.json",
        "modelscope@nndeploy/nndeploy:qwen/Qwen2-0.5B-Instruct/tokenizer.txt",
        "modelscope@nndeploy/nndeploy:qwen/Qwen2-0.5B-Instruct/embeddings_bf16.bin",
        "modelscope@nndeploy/nndeploy:qwen/Qwen2-0.5B-Instruct/tokenizer.json"
    ],
    "other_url_": [
        "template[http,modelscope]@https://template.cn/template.txt"
    ],
    "node_repository_": [
        {
            "key_": "nndeploy::qwen::QwenPrefill",
            "name_": "QwenPrefill_2",
            "desc_": "llm prefill stage [TokenizerText -> {token_ids, kv_}]",
            "device_type_": "kDeviceTypeCodeCpu:0",
            "is_dynamic_input_": false,
            "inputs_": [
                {
                    "desc_": "input_0",
                    "name_": "PromptNode_8@output_0",
                    "type_": "TokenizerText"
                }
            ],
            "is_dynamic_output_": false,
            "outputs_": [
                {
                    "desc_": "output_0",
                    "name_": "QwenPrefill_2@output_0",
                    "type_": "TokenizerIds"
                },
                {
                    "desc_": "output_1",
                    "name_": "QwenPrefill_2@output_1",
                    "type_": "Tensor"
                },
                {
                    "desc_": "output_2",
                    "name_": "QwenPrefill_2@output_2",
                    "type_": "TokenizerIds"
                }
            ],
            "is_composite_node_": true,
            "node_type_": "Intermediate",
            "config_path_": "resources/models/qwen/Qwen2-0.5B-Instruct/llm_config.json",
            "size": {
                "width": 200,
                "height": 80
            },
            "node_repository_": [
                {
                    "key_": "nndeploy::tokenizer::TokenizerEncodeCpp",
                    "name_": "token_node",
                    "desc_": "A tokenizer encode node that uses the C++ tokenizers library to encode text into token IDs. Supports HuggingFace and BPE tokenizers. Can encode single strings or batches of text. Provides vocabulary lookup and token-to-ID conversion.",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "is_dynamic_input_": false,
                    "inputs_": [
                        {
                            "desc_": "input_0",
                            "type_": "TokenizerText"
                        }
                    ],
                    "is_dynamic_output_": false,
                    "outputs_": [
                        {
                            "desc_": "output_0",
                            "type_": "TokenizerIds"
                        }
                    ],
                    "node_type_": "Intermediate",
                    "param_": {
                        "is_path_": true,
                        "tokenizer_type_": "kTokenizerTypeNotSupport",
                        "json_blob_": "",
                        "model_blob_": "",
                        "vocab_blob_": "",
                        "merges_blob_": "",
                        "added_tokens_": "",
                        "max_length_": 77
                    },
                    "node_repository_": []
                },
                {
                    "key_": "nndeploy::qwen::PrefillEmbeddingNode",
                    "name_": "embedding_node",
                    "desc_": "PrefillEmbeddingNode generates model input embeddings including:\n1. Token embedding vectors\n2. Attention mask matrix\n3. Position ids vector\n4. Past key values cache\n\nInputs:\n- inputs[0]: TokenizerIds containing input token sequence\nOutputs:\n- outputs[0]: Input token embedding tensor\n- outputs[1]: Attention mask tensor\n- outputs[2]: Position ids tensor\n- outputs[3]: Past key values cache tensor",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "is_dynamic_input_": false,
                    "inputs_": [
                        {
                            "desc_": "input_0",
                            "type_": "TokenizerIds"
                        }
                    ],
                    "is_dynamic_output_": false,
                    "outputs_": [
                        {
                            "desc_": "output_0",
                            "type_": "Tensor"
                        },
                        {
                            "desc_": "output_1",
                            "type_": "Tensor"
                        },
                        {
                            "desc_": "output_2",
                            "type_": "Tensor"
                        },
                        {
                            "desc_": "output_3",
                            "type_": "Tensor"
                        }
                    ],
                    "node_type_": "Intermediate",
                    "param_": {
                        "hidden_size_": 0,
                        "all_seq_len_": 0,
                        "gen_seq_len_": 0,
                        "embedding_file_": ""
                    },
                    "node_repository_": []
                },
                {
                    "key_": "nndeploy::infer::Infer",
                    "name_": "prefill_infer",
                    "desc_": "Universal Inference Node - Enables cross-platform model deployment with multiple inference backends while maintaining native performance",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "is_dynamic_input_": true,
                    "inputs_": [
                        {
                            "desc_": "input_0",
                            "type_": "Tensor"
                        },
                        {
                            "desc_": "input_1",
                            "type_": "NotSet"
                        },
                        {
                            "desc_": "input_2",
                            "type_": "NotSet"
                        },
                        {
                            "desc_": "input_3",
                            "type_": "NotSet"
                        }
                    ],
                    "is_dynamic_output_": true,
                    "outputs_": [
                        {
                            "desc_": "output_0",
                            "type_": "Tensor"
                        },
                        {
                            "desc_": "output_1",
                            "type_": "NotSet"
                        }
                    ],
                    "node_type_": "Intermediate",
                    "type_": "kInferenceTypeOnnxRuntime",
                    "param_": {
                        "model_type_": "kModelTypeOnnx",
                        "is_path_": true,
                        "model_value_": [
                            ""
                        ],
                        "external_model_data_": [
                            "resources/models/qwen/Qwen2-0.5B-Instruct/onnx/llm.onnx.data"
                        ],
                        "device_type_": "kDeviceTypeCodeCpu:0",
                        "num_thread_": 8,
                        "gpu_tune_kernel_": 1,
                        "input_num_": 1,
                        "input_name_": [
                            ""
                        ],
                        "input_shape_": [
                            [
                                -1,
                                -1,
                                -1,
                                -1
                            ]
                        ],
                        "output_num_": 1,
                        "output_name_": [
                            ""
                        ],
                        "encrypt_type_": "kEncryptTypeNone",
                        "license_": "",
                        "share_memory_mode_": "kShareMemoryTypeNoShare",
                        "precision_type_": "kPrecisionTypeFp32",
                        "power_type_": "kPowerTypeNormal",
                        "is_dynamic_shape_": false,
                        "min_shape_": {
                            "input_0": [
                                -1,
                                -1,
                                -1,
                                -1
                            ]
                        },
                        "opt_shape_": {
                            "input_0": [
                                -1,
                                -1,
                                -1,
                                -1
                            ]
                        },
                        "max_shape_": {
                            "input_0": [
                                -1,
                                -1,
                                -1,
                                -1
                            ]
                        },
                        "parallel_type_": "kParallelTypeNone",
                        "worker_num_": 1
                    },
                    "node_repository_": []
                },
                {
                    "key_": "nndeploy::qwen::PrefillSampleNode",
                    "name_": "prefill_sample_node",
                    "desc_": "llm sample node [logits -> token_ids]",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "is_dynamic_input_": false,
                    "inputs_": [
                        {
                            "desc_": "input_0",
                            "type_": "Tensor"
                        },
                        {
                            "desc_": "input_1",
                            "type_": "TokenizerIds"
                        }
                    ],
                    "is_dynamic_output_": false,
                    "outputs_": [
                        {
                            "desc_": "output_0",
                            "type_": "TokenizerIds"
                        }
                    ],
                    "node_type_": "Intermediate",
                    "node_repository_": []
                }
            ]
        },
        {
            "key_": "nndeploy::qwen::QwenDecode",
            "name_": "QwenDecode_3",
            "desc_": "llm decode stage [token_ids -> TokenizerText]",
            "device_type_": "kDeviceTypeCodeCpu:0",
            "is_dynamic_input_": false,
            "inputs_": [
                {
                    "desc_": "input_0",
                    "name_": "QwenPrefill_2@output_0",
                    "type_": "TokenizerIds"
                },
                {
                    "desc_": "input_1",
                    "name_": "QwenPrefill_2@output_1",
                    "type_": "Tensor"
                },
                {
                    "desc_": "input_2",
                    "name_": "QwenPrefill_2@output_2",
                    "type_": "TokenizerIds"
                }
            ],
            "is_dynamic_output_": false,
            "outputs_": [
                {
                    "desc_": "output_0",
                    "name_": "QwenDecode_3@output_0",
                    "type_": "TokenizerText"
                }
            ],
            "is_composite_node_": true,
            "node_type_": "Intermediate",
            "config_path_": "resources/models/qwen/Qwen2-0.5B-Instruct/llm_config.json",
            "size": {
                "width": 200,
                "height": 80
            },
            "node_repository_": [
                {
                    "key_": "nndeploy::qwen::DecodeEmbeddingNode",
                    "name_": "embedding_node",
                    "desc_": "DecodeEmbeddingNode generates model input embeddings including:\n1. Token embedding vectors\n2. Attention mask matrix\n3. Position ids vector\n4. Past key values cache\n\nInputs:\n- inputs[0]: TokenizerIds containing input token sequence\n- inputs[1]: past kv values\n- inputs[2]: history input token sequence\nOutputs:\n- outputs[0]: Input token embedding tensor\n- outputs[1]: Attention mask tensor\n- outputs[2]: Position ids tensor\n- outputs[3]: Past key values cache tensor",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "is_dynamic_input_": false,
                    "inputs_": [
                        {
                            "desc_": "input_0",
                            "type_": "TokenizerIds"
                        },
                        {
                            "desc_": "input_1",
                            "type_": "TokenizerIds"
                        },
                        {
                            "desc_": "input_2",
                            "type_": "Tensor"
                        }
                    ],
                    "is_dynamic_output_": false,
                    "outputs_": [
                        {
                            "desc_": "output_0",
                            "type_": "Tensor"
                        },
                        {
                            "desc_": "output_1",
                            "type_": "Tensor"
                        },
                        {
                            "desc_": "output_2",
                            "type_": "Tensor"
                        },
                        {
                            "desc_": "output_3",
                            "type_": "Tensor"
                        }
                    ],
                    "node_type_": "Intermediate",
                    "param_": {
                        "hidden_size_": 0,
                        "all_seq_len_": 0,
                        "gen_seq_len_": 0,
                        "embedding_file_": ""
                    },
                    "node_repository_": []
                },
                {
                    "key_": "nndeploy::infer::Infer",
                    "name_": "decode_infer",
                    "desc_": "Universal Inference Node - Enables cross-platform model deployment with multiple inference backends while maintaining native performance",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "is_dynamic_input_": true,
                    "inputs_": [
                        {
                            "desc_": "input_0",
                            "type_": "Tensor"
                        },
                        {
                            "desc_": "input_1",
                            "type_": "NotSet"
                        },
                        {
                            "desc_": "input_2",
                            "type_": "NotSet"
                        },
                        {
                            "desc_": "input_3",
                            "type_": "NotSet"
                        }
                    ],
                    "is_dynamic_output_": true,
                    "outputs_": [
                        {
                            "desc_": "output_0",
                            "type_": "Tensor"
                        },
                        {
                            "desc_": "output_1",
                            "type_": "NotSet"
                        }
                    ],
                    "node_type_": "Intermediate",
                    "type_": "kInferenceTypeOnnxRuntime",
                    "param_": {
                        "model_type_": "kModelTypeOnnx",
                        "is_path_": true,
                        "model_value_": [
                            "resources/models/qwen/Qwen2-0.5B-Instruct/onnx/llm.onnx"
                        ],
                        "external_model_data_": [
                            "resources/models/qwen/Qwen2-0.5B-Instruct/onnx/llm.onnx.data"
                        ],
                        "device_type_": "kDeviceTypeCodeCpu:0",
                        "num_thread_": 8,
                        "gpu_tune_kernel_": 1,
                        "input_num_": 1,
                        "input_name_": [
                            ""
                        ],
                        "input_shape_": [
                            [
                                -1,
                                -1,
                                -1,
                                -1
                            ]
                        ],
                        "output_num_": 1,
                        "output_name_": [
                            ""
                        ],
                        "encrypt_type_": "kEncryptTypeNone",
                        "license_": "",
                        "share_memory_mode_": "kShareMemoryTypeNoShare",
                        "precision_type_": "kPrecisionTypeFp32",
                        "power_type_": "kPowerTypeNormal",
                        "is_dynamic_shape_": false,
                        "min_shape_": {
                            "input_0": [
                                -1,
                                -1,
                                -1,
                                -1
                            ]
                        },
                        "opt_shape_": {
                            "input_0": [
                                -1,
                                -1,
                                -1,
                                -1
                            ]
                        },
                        "max_shape_": {
                            "input_0": [
                                -1,
                                -1,
                                -1,
                                -1
                            ]
                        },
                        "parallel_type_": "kParallelTypeNone",
                        "worker_num_": 1
                    },
                    "node_repository_": []
                },
                {
                    "key_": "nndeploy::qwen::DecodeSampleNode",
                    "name_": "sample_node",
                    "desc_": "llm sample node [logits -> token_ids]",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "is_dynamic_input_": false,
                    "inputs_": [
                        {
                            "desc_": "input_0",
                            "type_": "Tensor"
                        }
                    ],
                    "is_dynamic_output_": false,
                    "outputs_": [
                        {
                            "desc_": "output_0",
                            "type_": "TokenizerIds"
                        }
                    ],
                    "node_type_": "Intermediate",
                    "param_": {},
                    "node_repository_": []
                },
                {
                    "key_": "nndeploy::tokenizer::TokenizerDecodeCpp",
                    "name_": "decode_node",
                    "desc_": "A tokenizer decode node that uses the C++ tokenizers library to decode token IDs into text. Supports HuggingFace and BPE tokenizers. Can decode single token IDs or batches of token IDs. Provides token-to-text conversion.",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "is_dynamic_input_": false,
                    "inputs_": [
                        {
                            "desc_": "input_0",
                            "type_": "TokenizerIds"
                        }
                    ],
                    "is_dynamic_output_": false,
                    "outputs_": [
                        {
                            "desc_": "output_0",
                            "type_": "TokenizerText"
                        }
                    ],
                    "node_type_": "Intermediate",
                    "param_": {
                        "is_path_": true,
                        "tokenizer_type_": "kTokenizerTypeNotSupport",
                        "json_blob_": "",
                        "model_blob_": "",
                        "vocab_blob_": "",
                        "merges_blob_": "",
                        "added_tokens_": "",
                        "max_length_": 77
                    },
                    "node_repository_": []
                }
            ]
        },
        {
            "key_": "nndeploy::qwen::PrintNode",
            "name_": "PrintNode_7",
            "developer_": "",
            "source_": "",
            "desc_": "Print TokenizerText content and save to temporary output file.",
            "device_type_": "kDeviceTypeCodeCpu:0",
            "version_": "1.0.0",
            "required_params_": [
                "path_"
            ],
            "ui_params_": [],
            "io_params_": [],
            "dropdown_params_": {},
            "is_dynamic_input_": false,
            "inputs_": [
                {
                    "desc_": "input_0",
                    "name_": "QwenDecode_3@output_0",
                    "type_": "TokenizerText"
                }
            ],
            "is_dynamic_output_": false,
            "outputs_": [],
            "node_type_": "Output",
            "io_type_": "Text",
            "path_": "resources/others/qwen.txt",
            "size": {
                "width": 200,
                "height": 80
            },
            "node_repository_": []
        },
        {
            "key_": "nndeploy::qwen::PromptNode",
            "name_": "PromptNode_8",
            "developer_": "",
            "source_": "",
            "desc_": "Generate TokenizerText from prompt string using optional template.",
            "device_type_": "kDeviceTypeCodeCpu:0",
            "version_": "1.0.0",
            "required_params_": [],
            "ui_params_": [],
            "io_params_": [],
            "dropdown_params_": {},
            "is_dynamic_input_": false,
            "inputs_": [],
            "is_dynamic_output_": false,
            "outputs_": [
                {
                    "desc_": "output_0",
                    "name_": "PromptNode_8@output_0",
                    "type_": "TokenizerText"
                }
            ],
            "node_type_": "Input",
            "io_type_": "String",
            "param_": {
                "required_params_": [
                    "user_content_"
                ],
                "ui_params_": [],
                "io_params_": [],
                "dropdown_params_": {},
                "prompt_template_": "<|im_start|>user\n%s<|im_end|>\n<|im_start|>assistant\n",
                "user_content_": "谁是李白"
            },
            "size": {
                "width": 200,
                "height": 80
            },
            "node_repository_": []
        }
    ],
    "nndeploy_ui_layout": {
        "layout": {
            "QwenPrefill_2": {
                "position": {
                    "x": 330,
                    "y": 55.900000000000006
                },
                "size": {
                    "width": 200,
                    "height": 80
                },
                "expanded": true,
                "children": {
                    "token_node": {
                        "position": {
                            "x": 100,
                            "y": 47.167547246773374
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    },
                    "embedding_node": {
                        "position": {
                            "x": 342.9291322909654,
                            "y": 0
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    },
                    "prefill_infer": {
                        "position": {
                            "x": 585.8582645819308,
                            "y": 0
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    },
                    "prefill_sample_node": {
                        "position": {
                            "x": 816.5579252209602,
                            "y": 36.267547246773375
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    }
                }
            },
            "QwenDecode_3": {
                "position": {
                    "x": 1560,
                    "y": 67.89999999999998
                },
                "size": {
                    "width": 200,
                    "height": 80
                },
                "expanded": true,
                "children": {
                    "embedding_node": {
                        "position": {
                            "x": 100,
                            "y": 0
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    },
                    "decode_infer": {
                        "position": {
                            "x": 400,
                            "y": 0
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    },
                    "sample_node": {
                        "position": {
                            "x": 700,
                            "y": 32.64999999999999
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    },
                    "decode_node": {
                        "position": {
                            "x": 1000,
                            "y": 32.64999999999999
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    }
                }
            },
            "PrintNode_7": {
                "position": {
                    "x": 2860,
                    "y": 87.55000000000001
                },
                "size": {
                    "width": 200,
                    "height": 80
                },
                "expanded": true
            },
            "PromptNode_8": {
                "position": {
                    "x": 100,
                    "y": 66.75
                },
                "size": {
                    "width": 200,
                    "height": 80
                },
                "expanded": true
            }
        },
        "groups": []
    }
}